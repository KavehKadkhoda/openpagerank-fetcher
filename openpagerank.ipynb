{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB3RiN8K4g9NPC4E5QRVyP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bulk Open PageRank Fetcher — Developer & Operator Guide**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1  What this module does\n",
        "\n",
        "* Queries the **Open PageRank (OPR) REST API** for PageRank®‐like scores.\n",
        "* Accepts any mixture of URLs, host‑names or raw strings, extracts **valid registrable domains** and ignores the rest.\n",
        "* Caches results locally (`cache.json`) and refreshes them **only when OPR publishes a newer dataset**.\n",
        "* Obeys the service’s quotas automatically:\n",
        "\n",
        "  * ≤ 10 000 HTTP calls per rolling hour.\n",
        "  * ≤ 4 300 000 domains retrieved per rolling 24 h window.\n",
        "* Runs a **background thread** that can refresh the entire cache after a dataset update, while still serving the foreground request.\n",
        "\n",
        "---\n",
        "\n",
        "## 2  High‑level architecture\n",
        "\n",
        "```\n",
        "                +-------------+\n",
        "input strings → | extractor   |-- invalid → drop\n",
        "                +-------------+\n",
        "                       │ valid domains\n",
        "                       ▼\n",
        "               +------------------+\n",
        "               | cache look‑up    |\n",
        "               +------------------+\n",
        "          hit  │              │ miss /\n",
        "               │              │ stale\n",
        "               ▼              ▼\n",
        "      return cached   +-------------------+\n",
        "         (O(µs))      | fetch_opr_batch   |—→ OPR API\n",
        "                      +-------------------+\n",
        "                             │\n",
        "                             ▼\n",
        "                    +------------------+\n",
        "                    | quota checkers   |  (hourly / daily)\n",
        "                    +------------------+\n",
        "                             │\n",
        "                             ▼\n",
        "                 update cache & return result\n",
        "```\n",
        "\n",
        "A separate scheduler thread periodically calls the same `fetch_opr_batch` in **chunks of 100** until every stale record is refreshed, throttled by the same quota monitors.\n",
        "\n",
        "---\n",
        "\n",
        "## 3  Installation prerequisites\n",
        "\n",
        "| Component | Version / Note                       |\n",
        "| --------- | ------------------------------------ |\n",
        "| Python    |  3.8 +                               |\n",
        "| Packages  |  `requests` *(pip install requests)* |\n",
        "\n",
        "No external databases or message brokers are required; the cache is a single JSON file kept next to the script.\n",
        "\n",
        "---\n",
        "\n",
        "## 4  Configuration\n",
        "\n",
        "| Name                  | Where                                                                | Meaning                                                                       |\n",
        "| --------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| `API_KEY`             | hard‑coded in `__main__` or supplied from an env‑var that you inject | Your personal key from [https://openpagerank.com/](https://openpagerank.com/) |\n",
        "| `cache.json`          | same directory as the script (default)                               | Persistent store for all domains already fetched                              |\n",
        "| `MAX_CALLS_PER_HOUR`  | module constant                                                      | Leave at 10 000 unless OPR changes its policy                                 |\n",
        "| `MAX_DOMAINS_PER_DAY` | module constant                                                      | Leave at 4 300 000                                                            |\n",
        "| `DOMAINS_PER_CALL`    | module constant                                                      | Must stay at 100 (API limit)                                                  |\n",
        "\n",
        "---\n",
        "\n",
        "## 5  Public entry points\n",
        "\n",
        "### 5.1 `process_urls_in_batches(...)`\n",
        "\n",
        "| Parameter   | Type              | Description                                                   |\n",
        "| ----------- | ----------------- | ------------------------------------------------------------- |\n",
        "| `urls`      | `List[Any]`       | Arbitrary list of strings / URLs / host‑names                 |\n",
        "| `cache`     | `Dict[str, dict]` | The in‑memory cache object (usually loaded by `load_cache()`) |\n",
        "| `api_key`   | `str`             | Your OPR key                                                  |\n",
        "| `global_lu` | `str`             | Latest **global** dataset date (`YYYY‑MM‑DD`)                 |\n",
        "\n",
        "**Returns** `Dict[str, Any]` – mapping every *valid* domain found in `urls` to a float PageRank value or `None` if the API had no data.\n",
        "\n",
        "> Use this when you want a **synchronous** answer for an ad‑hoc list of URLs.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 `schedule_full_refresh(...)`\n",
        "\n",
        "Spawns a **daemon thread** that calls `refresh_full_dataset` and then `save_cache`. You normally call this once at application start‑up *after* you have learnt `global_lu` from a cheap single‑domain probe (`google.com` is used in the template).\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Helper utilities\n",
        "\n",
        "| Function                                       | Purpose                                                                                 |\n",
        "| ---------------------------------------------- | --------------------------------------------------------------------------------------- |\n",
        "| `load_cache(path=\"cache.json\")` / `save_cache` | JSON persistence                                                                        |\n",
        "| `extract_domain(obj)`                          | Converts heterogenous input to a bare domain or returns `None`                          |\n",
        "| `is_valid_domain(s)`                           | RFC‑ish regex check                                                                     |\n",
        "| `parse_opr_date(s)`                            | Converts OPR’s `last_updated` header (“25th Dec 2024” or “1st June 2025”) into ISO date |\n",
        "\n",
        "---\n",
        "\n",
        "## 6  How to embed in your own project\n",
        "\n",
        "```python\n",
        "from opr_fetcher import (\n",
        "    load_cache, save_cache,\n",
        "    fetch_opr_batch, process_urls_in_batches,\n",
        "    schedule_full_refresh\n",
        ")\n",
        "\n",
        "API_KEY = os.getenv(\"OPR_KEY\")\n",
        "\n",
        "cache = load_cache()\n",
        "\n",
        "# discover latest dataset date once\n",
        "global_lu = fetch_opr_batch([\"google.com\"], API_KEY)[\"google.com\"][\"last_updated\"]\n",
        "\n",
        "# background auto‑refresh (non‑blocking)\n",
        "schedule_full_refresh(cache, API_KEY, global_lu)\n",
        "\n",
        "# anywhere in your code:\n",
        "urls = [\"https://www.nytimes.com\", \"foo\", \"bar.co.uk/?x=1\"]\n",
        "scores = process_urls_in_batches(urls, cache, API_KEY, global_lu)\n",
        "print(scores)       # {'nytimes.com': 7.9, 'bar.co.uk': 4.4}\n",
        "\n",
        "# flush cache to disk on app shutdown\n",
        "save_cache(cache)\n",
        "```\n",
        "\n",
        "You can safely run multiple foreground batches per process; all share the same thread‑safe quota counters.\n",
        "\n",
        "---\n",
        "\n",
        "## 7  Operational characteristics\n",
        "\n",
        "| Aspect                | Detail                                                                                                                                                                                                                |\n",
        "| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Throughput**        | Up to 1 000 000 domains per hour (10 000 calls × 100 domains) until the daily 4.3 M cap is hit.                                                                                                                       |\n",
        "| **Thread‑safety**     | Internal counters (`RL_LOCK`) protect against races **inside one Python process**. If you run multiple processes or containers, use an external lock (Redis, DB row) around `check_rate_limit` & `check_daily_limit`. |\n",
        "| **Idempotency**       | The cache ensures duplicate domains are fetched once per dataset version.                                                                                                                                             |\n",
        "| **Failure behaviour** | Network errors: 3 attempts with 5 s back‑off, then exception; the offending batch records `page_rank=None`. Parsing errors: logged and default to today’s date.                                                       |\n",
        "| **Disk usage**        | Roughly 90 B per domain in `cache.json`. One million domains ≈ 85 MB.                                                                                                                                                 |\n",
        "\n",
        "---\n",
        "\n",
        "## 8  Typical console session\n",
        "\n",
        "```\n",
        "$ python opr_fetcher.py\n",
        "[INFO] Fetching OPR global last_updated via 'google.com' …\n",
        "[INFO] Global OPR last_updated = 2025-06-01\n",
        "[SCHEDULER] Cache already fresh – nothing to do.\n",
        "[INFO] Processing request …\n",
        "   example.com                    → 6.56\n",
        "   kaveh.com                      → 0\n",
        "   fbk.eu                         → 4.53\n",
        "   hafez.it                       → 2.01\n",
        "   google.com                     → 10\n",
        "[INFO] Done.\n",
        "[INFO] Cache saved; background refresh continues (if needed).\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9  Extending the module\n",
        "\n",
        "* **Custom persistence** — swap `load_cache`/`save_cache` with a Postgres table or Redis hash if you need multi‑process coherence.\n",
        "* **Advanced scheduling** — replace the bare thread with `APScheduler`, Celery beat, or a cron job if you prefer.\n",
        "* **Metrics & logging** — wrap log prints with your favourite logger and export Prometheus counters from `RATE_LIMIT` and `DAILY_LIMIT`.\n",
        "* **More validation rules** — adjust `DOMAIN_RE` for stricter or laxer hostnames (IDN Unicode, private TLDs, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## 10  Limitations & caveats\n",
        "\n",
        "* Designed around **OpenPageRank’s** current limits (100‑domain batch, 10k calls/h, 4.3 M domains/day). If the service changes these, update the constants.\n",
        "* Single‑threaded quota tracking; separate OS processes will not coordinate without extra work.\n",
        "* Relies on the OPR header `last_updated`; if the API removes or renames it, `parse_opr_date` will need an update.\n",
        "\n",
        "---\n",
        "\n",
        "Happy ranking!\n"
      ],
      "metadata": {
        "id": "SrdFI8c95Afv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Bulk Open‑PageRank fetcher with caching, per‑hour and per‑day limits,\n",
        "background full refresh, and strict domain validation.\n",
        "\n",
        "Author: Kaveh\n",
        "Updated: 2025‑06‑16\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import re\n",
        "import threading\n",
        "import time\n",
        "from typing import Any, Dict, List, Union\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import requests\n",
        "\n",
        "# ------------------------- #\n",
        "#   RATE‑ & QUOTA LIMITS    #\n",
        "# ------------------------- #\n",
        "MAX_CALLS_PER_HOUR = 10_000                  # OPR limit\n",
        "MAX_DOMAINS_PER_DAY = 4_300_000              # OPR daily dataset limit\n",
        "DOMAINS_PER_CALL = 100                       # OPR batch size\n",
        "\n",
        "RATE_LIMIT = {\n",
        "    \"start_time\": time.time(),               # start of the current hour window\n",
        "    \"calls\": 0                               # successful calls in this window\n",
        "}\n",
        "\n",
        "DAILY_LIMIT = {\n",
        "    \"day_start\": time.time(),                # start of current 24‑h window\n",
        "    \"domains\": 0                             # domains fetched in this window\n",
        "}\n",
        "\n",
        "RL_LOCK = threading.Lock()  # ensures atomic updates when threads are used\n",
        "\n",
        "\n",
        "def check_rate_limit() -> None:\n",
        "    \"\"\"Block if the script would exceed 10 000 calls within an hour.\"\"\"\n",
        "    with RL_LOCK:\n",
        "        now = time.time()\n",
        "        elapsed = now - RATE_LIMIT[\"start_time\"]\n",
        "\n",
        "        # reset hourly window\n",
        "        if elapsed >= 3600:\n",
        "            RATE_LIMIT.update(start_time=now, calls=0)\n",
        "\n",
        "        # wait out the hour if quota exhausted\n",
        "        if RATE_LIMIT[\"calls\"] >= MAX_CALLS_PER_HOUR:\n",
        "            remaining = int(3600 - elapsed)\n",
        "            print(f\"[RATE LIMIT] Hourly quota reached — sleeping {remaining} s …\")\n",
        "            time.sleep(max(0, remaining))\n",
        "            RATE_LIMIT.update(start_time=time.time(), calls=0)\n",
        "\n",
        "        RATE_LIMIT[\"calls\"] += 1\n",
        "\n",
        "\n",
        "def check_daily_limit(domains_to_fetch: int) -> None:\n",
        "    \"\"\"Block if the script would exceed 4.3 M domains within the current day.\"\"\"\n",
        "    with RL_LOCK:\n",
        "        now = time.time()\n",
        "        elapsed = now - DAILY_LIMIT[\"day_start\"]\n",
        "\n",
        "        # reset daily window\n",
        "        if elapsed >= 86_400:\n",
        "            DAILY_LIMIT.update(day_start=now, domains=0)\n",
        "\n",
        "        # wait out the day if quota exhausted\n",
        "        if DAILY_LIMIT[\"domains\"] + domains_to_fetch > MAX_DOMAINS_PER_DAY:\n",
        "            remaining = int(86_400 - elapsed)\n",
        "            print(f\"[DAILY LIMIT] Daily quota reached — sleeping {remaining} s …\")\n",
        "            time.sleep(max(0, remaining))\n",
        "            DAILY_LIMIT.update(day_start=time.time(), domains=0)\n",
        "\n",
        "        DAILY_LIMIT[\"domains\"] += domains_to_fetch\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#     HELPER – DATE CONVERSION     #\n",
        "# -------------------------------- #\n",
        "def parse_opr_date(date_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert Open‑PageRank 'last_updated' strings to 'YYYY‑MM‑DD'.\n",
        "\n",
        "    Handles both abbreviated months ('25th Dec 2024') and full month\n",
        "    names ('1st June 2025').  If parsing fails, today's date is returned.\n",
        "    \"\"\"\n",
        "    # Split into components and strip ordinal suffixes\n",
        "    try:\n",
        "        day, mon, year = date_str.split()\n",
        "    except ValueError:\n",
        "        print(f\"[WARN] Could not parse OPR date '{date_str}' (unexpected shape)\")\n",
        "        return datetime.date.today().isoformat()\n",
        "\n",
        "    day = re.sub(r\"(st|nd|rd|th)$\", \"\", day)\n",
        "    cleaned = f\"{day} {mon} {year}\"\n",
        "\n",
        "    # Try both formats: abbreviated month (%b) and full month (%B)\n",
        "    for fmt in (\"%d %b %Y\", \"%d %B %Y\"):\n",
        "        try:\n",
        "            return datetime.datetime.strptime(cleaned, fmt).strftime(\"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    # Last resort\n",
        "    print(f\"[WARN] Could not parse OPR date '{date_str}'\")\n",
        "    return datetime.date.today().isoformat()\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#       HELPER – HTTP RETRIES      #\n",
        "# -------------------------------- #\n",
        "def safe_get(url: str,\n",
        "             params: Dict[str, Any],\n",
        "             headers: Dict[str, str]) -> requests.Response:\n",
        "    \"\"\"GET wrapper with three attempts and 5‑second back‑off.\"\"\"\n",
        "    backoff, retries = 5, 3\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            resp = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "            resp.raise_for_status()\n",
        "            return resp\n",
        "        except requests.exceptions.RequestException as exc:\n",
        "            if attempt < retries:\n",
        "                print(f\"[WARN] Attempt {attempt} failed ({exc}) – retrying in \"\n",
        "                      f\"{backoff}s …\")\n",
        "                time.sleep(backoff)\n",
        "            else:\n",
        "                print(f\"[ERROR] GET failed after {retries} attempts: {exc}\")\n",
        "                raise\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#     DOMAIN VALIDATION & PARSE    #\n",
        "# -------------------------------- #\n",
        "DOMAIN_RE = re.compile(\n",
        "    # e.g. example.co.uk, foo-bar.io, xn--bcher-kva.ch (IDN punycode)\n",
        "    r\"^(?:[a-z0-9-]{1,63}\\.)+[a-z0-9-]{2,63}$\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "\n",
        "def is_valid_domain(domain: str) -> bool:\n",
        "    \"\"\"Return True if the string looks like a real domain.\"\"\"\n",
        "    return bool(DOMAIN_RE.match(domain))\n",
        "\n",
        "\n",
        "def extract_domain(url_candidate: Any) -> Union[str, None]:\n",
        "    \"\"\"\n",
        "    Parse the candidate into a bare domain.\n",
        "    Returns None if the candidate is not a valid registrable domain.\n",
        "    \"\"\"\n",
        "    if not isinstance(url_candidate, str):\n",
        "        return None\n",
        "\n",
        "    candidate = url_candidate.strip()\n",
        "    if not candidate:\n",
        "        return None\n",
        "\n",
        "    # prepend scheme for urlparse\n",
        "    candidate = candidate if \"://\" in candidate else f\"http://{candidate}\"\n",
        "\n",
        "    try:\n",
        "        netloc = urlparse(candidate).netloc.lower()\n",
        "        if netloc.startswith(\"www.\"):\n",
        "            netloc = netloc[4:]\n",
        "        return netloc if is_valid_domain(netloc) else None\n",
        "    except Exception:  # noqa: BLE001\n",
        "        return None\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#         BATCH OPR FETCH          #\n",
        "# -------------------------------- #\n",
        "def fetch_opr_batch(domains: List[str],\n",
        "                    api_key: str) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fetch PageRank for ≤ 100 domains in one call.\n",
        "    Returns {domain: {\"page_rank\", \"last_updated\"}}.\n",
        "    \"\"\"\n",
        "    if len(domains) > DOMAINS_PER_CALL:\n",
        "        raise ValueError(\"fetch_opr_batch handles max 100 domains.\")\n",
        "\n",
        "    # observe quotas\n",
        "    check_daily_limit(len(domains))\n",
        "    check_rate_limit()\n",
        "\n",
        "    base_url = \"https://openpagerank.com/api/v1.0/getPageRank\"\n",
        "    params = [(\"domains[]\", d) for d in domains]\n",
        "    headers = {\"API-OPR\": api_key}\n",
        "    result: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    try:\n",
        "        data = safe_get(base_url, params, headers).json()\n",
        "        batch_lu = parse_opr_date(data.get(\"last_updated\", \"\"))\n",
        "\n",
        "        for item in data.get(\"response\", []):\n",
        "            domain = item.get(\"domain\")\n",
        "            result[domain] = {\n",
        "                \"page_rank\": item.get(\"page_rank_decimal\"),\n",
        "                \"last_updated\": batch_lu\n",
        "            }\n",
        "\n",
        "        # placeholders for domains missing in response\n",
        "        for d in domains:\n",
        "            result.setdefault(d, {\"page_rank\": None, \"last_updated\": batch_lu})\n",
        "\n",
        "    except Exception as exc:    # noqa: BLE001\n",
        "        print(f\"[ERROR] Batch fetch failed for {domains}: {exc}\")\n",
        "        today = datetime.date.today().isoformat()\n",
        "        for d in domains:\n",
        "            result[d] = {\"page_rank\": None, \"last_updated\": today}\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#        CACHE FRESHNESS TEST      #\n",
        "# -------------------------------- #\n",
        "def should_refresh_domain(cached: Dict[str, Any],\n",
        "                          global_lu: str) -> bool:\n",
        "    \"\"\"True if cached record is older than OPR's global `last_updated`.\"\"\"\n",
        "    cached_lu = cached.get(\"opr_last_updated\")\n",
        "    if not cached_lu:\n",
        "        return True\n",
        "    try:\n",
        "        return (datetime.date.fromisoformat(cached_lu) <\n",
        "                datetime.date.fromisoformat(global_lu))\n",
        "    except ValueError:\n",
        "        return True\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#      MAIN BATCH PROCESSOR        #\n",
        "# -------------------------------- #\n",
        "def process_urls_in_batches(urls: List[Any],\n",
        "                            cache: Dict[str, Dict[str, Any]],\n",
        "                            api_key: str,\n",
        "                            global_lu: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Resolve a heterogeneous list of inputs to domains, pull PageRank where\n",
        "    required, update cache, and return {domain: page_rank}.\n",
        "    \"\"\"\n",
        "    results: Dict[str, Any] = {}\n",
        "    domains = {extract_domain(u) for u in urls}\n",
        "    domains.discard(None)  # remove invalids\n",
        "\n",
        "    fetch_list: List[str] = []\n",
        "    for d in domains:\n",
        "        c = cache.get(d)\n",
        "        if c and not should_refresh_domain(c, global_lu):\n",
        "            results[d] = c[\"page_rank_decimal\"]\n",
        "        else:\n",
        "            fetch_list.append(d)\n",
        "\n",
        "    # fetch in chunks\n",
        "    for i in range(0, len(fetch_list), DOMAINS_PER_CALL):\n",
        "        chunk = fetch_list[i:i + DOMAINS_PER_CALL]\n",
        "        batch = fetch_opr_batch(chunk, api_key)\n",
        "        today = datetime.date.today().isoformat()\n",
        "\n",
        "        for d in chunk:\n",
        "            pr = batch[d][\"page_rank\"]\n",
        "            lu = batch[d][\"last_updated\"]\n",
        "            cache[d] = {\n",
        "                \"page_rank_decimal\": pr,\n",
        "                \"last_checked\": today,\n",
        "                \"opr_last_updated\": lu\n",
        "            }\n",
        "            results[d] = pr\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#       FULL‑DATASET REFRESH       #\n",
        "# -------------------------------- #\n",
        "def refresh_full_dataset(cache: Dict[str, Dict[str, Any]],\n",
        "                         api_key: str,\n",
        "                         global_lu: str) -> None:\n",
        "    \"\"\"\n",
        "    Refresh every domain in cache that is older than the current global_lu.\n",
        "    Respects hourly and daily quotas; may take hours to finish.\n",
        "    \"\"\"\n",
        "    stale = [d for d, rec in cache.items()\n",
        "             if should_refresh_domain(rec, global_lu)]\n",
        "\n",
        "    if not stale:\n",
        "        print(\"[SCHEDULER] Cache already fresh – nothing to do.\")\n",
        "        return\n",
        "\n",
        "    print(f\"[SCHEDULER] {len(stale):,} stale domains to refresh …\")\n",
        "    for i in range(0, len(stale), DOMAINS_PER_CALL):\n",
        "        chunk = stale[i:i + DOMAINS_PER_CALL]\n",
        "        batch = fetch_opr_batch(chunk, api_key)\n",
        "        today = datetime.date.today().isoformat()\n",
        "\n",
        "        for d in chunk:\n",
        "            pr = batch[d][\"page_rank\"]\n",
        "            lu = batch[d][\"last_updated\"]\n",
        "            cache[d] = {\n",
        "                \"page_rank_decimal\": pr,\n",
        "                \"last_checked\": today,\n",
        "                \"opr_last_updated\": lu\n",
        "            }\n",
        "\n",
        "        # optional: periodically save progress\n",
        "        if i % (DOMAINS_PER_CALL * 100) == 0:\n",
        "            save_cache(cache)\n",
        "\n",
        "    print(\"[SCHEDULER] Full refresh completed.\")\n",
        "\n",
        "\n",
        "def schedule_full_refresh(cache: Dict[str, Dict[str, Any]],\n",
        "                          api_key: str,\n",
        "                          global_lu: str) -> None:\n",
        "    \"\"\"Spawn a background thread to run `refresh_full_dataset`.\"\"\"\n",
        "    threading.Thread(\n",
        "        target=lambda: (refresh_full_dataset(cache, api_key, global_lu),\n",
        "                        save_cache(cache)),\n",
        "        daemon=True,\n",
        "        name=\"opr_full_refresh\"\n",
        "    ).start()\n",
        "\n",
        "\n",
        "# -------------------------------- #\n",
        "#         CACHE SERIALISERS        #\n",
        "# -------------------------------- #\n",
        "def load_cache(path: str = \"cache.json\") -> Dict[str, Dict[str, Any]]:\n",
        "    try:\n",
        "        with open(path, \"r\") as fh:\n",
        "            return json.load(fh)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return {}\n",
        "\n",
        "\n",
        "def save_cache(cache: Dict[str, Dict[str, Any]],\n",
        "               path: str = \"cache.json\") -> None:\n",
        "    with open(path, \"w\") as fh:\n",
        "        json.dump(cache, fh, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "_G7jN7hPBO6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------- #\n",
        "#        DEMO / ENTRY‑POINT        #\n",
        "# -------------------------------- #\n",
        "def example_usage(urls: List[Any],\n",
        "                  cache: Dict[str, Dict[str, Any]],\n",
        "                  api_key: str,\n",
        "                  global_lu: str) -> None:\n",
        "    print(\"[INFO] Processing request …\")\n",
        "    results = process_urls_in_batches(urls, cache, api_key, global_lu)\n",
        "\n",
        "    for domain, pr in results.items():\n",
        "        print(f\"   {domain:<30} → {pr}\")\n",
        "\n",
        "    print(\"[INFO] Done.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    API_KEY = \"Your OPR key\"   #\n",
        "\n",
        "    # 1) load existing cache\n",
        "    cache_dict = load_cache()\n",
        "\n",
        "    # 2) get current global last_updated using a cheap single‑domain call\n",
        "    print(\"[INFO] Fetching OPR global last_updated via 'google.com' …\")\n",
        "    global_lu = fetch_opr_batch([\"google.com\"], API_KEY)[\"google.com\"][\"last_updated\"]\n",
        "    print(f\"[INFO] Global OPR last_updated = {global_lu}\")\n",
        "\n",
        "    # 3) schedule the full‑dataset refresh (non‑blocking)\n",
        "    schedule_full_refresh(cache_dict, API_KEY, global_lu)\n",
        "\n",
        "    # 4) handle the “foreground” request\n",
        "    sample_inputs = [\n",
        "        \"https://www.google.com/search?q=somequery\",\n",
        "        \"hafez.it\",\n",
        "        \"fbk.eu\",\n",
        "        \"example.com\",\n",
        "        \"mail\",          # rejected by validator\n",
        "        \"www.kaveh.com\",\n",
        "        12345            # rejected – not str\n",
        "    ]\n",
        "    example_usage(sample_inputs, cache_dict, API_KEY, global_lu)\n",
        "\n",
        "    # 5) persist any immediate changes\n",
        "    save_cache(cache_dict)\n",
        "    print(\"[INFO] Cache saved; background refresh continues (if needed).\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ8KBcbgBO8f",
        "outputId": "abba9adc-a0e1-4969-e28e-1305687ab14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Fetching OPR global last_updated via 'google.com' …\n",
            "[INFO] Global OPR last_updated = 2025-06-01\n",
            "[SCHEDULER] Cache already fresh – nothing to do.\n",
            "[INFO] Processing request …\n",
            "   example.com                    → 6.56\n",
            "   kaveh.com                      → 0\n",
            "   fbk.eu                         → 4.53\n",
            "   hafez.it                       → 2.01\n",
            "   google.com                     → 10\n",
            "[INFO] Done.\n",
            "[INFO] Cache saved; background refresh continues (if needed).\n"
          ]
        }
      ]
    }
  ]
}